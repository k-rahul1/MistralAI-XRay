# Author 
# Rahul Kumar (Northeastern University)

import requests
import json
import pandas as pd
from tqdm import trange
import torch


def get_ollama_completion(prompt, model="mistral"):
    """
    Send a prompt to a local Ollama instance and retrieve the generated completion.

    This function makes an HTTP POST request to a local server running an Ollama model. It sends a JSON
    payload containing the model type and the prompt, and returns the model's generated textual response.

    Args:
    prompt (str): The prompt text to send to the model.
    model (str): The model identifier, defaulting to 'mistral'.

    Returns:
    str: The text generated by the model in response to the prompt.
    """
    url = "http://localhost:11434/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    response = requests.post(url, json=data)
    return json.loads(response.text)['response']



def generate_report(retrieved_documents, query, model="mistral"):
    """
    Generate a detailed report based on provided documents and a query using an Ollama model.

    This function processes a list of documents, combines them into a single context, and constructs a
    prompt for an AI model to generate a report. The function utilizes the 'get_ollama_completion' function
    to get the AI-generated text based on the constructed prompt.

    Args:
    retrieved_documents (list[str]): A list of document texts retrieved as relevant information.
    query (str): The query specifying what the report should focus on.
    model (str): The Ollama model identifier to use for generating the report, defaulting to 'mistral'.

    Returns:
    str: A string containing the AI-generated report, formatted based on the input query and context.
    """
    # Combine retrieved documents into a single context
    context = "\n\n".join(retrieved_documents)

    # Define system and user prompts
    system_prompt = "You are an assistant designed to generate radiology reports based on provided context and query."

    user_prompt = f"""Based on the following context and query, generate a detailed report:

                    Context:
                    {context}

                    Query: {query}
                    """

    # Combine prompts and generate the report
    full_prompt = f"System: {system_prompt}\n\nHuman: {user_prompt}\n\nAssistant:"
    
    report = get_ollama_completion(full_prompt, model)
    
    return report.strip()

if __name__ == "__main__":
    csv_path = 'Training/ALBEF/top_k_pred.csv'  # Replace with the path to your top k prediction CSV file
    df = pd.read_csv(csv_path)

    query = """Please generate a concise radiology report with only impression section using information only from retrieved context. 
    ##
    Follow these guidelines to generate the report:
    1) Do not provide report by bullet points or numbers.
    2) Do not write the word impression, provide only the report. 
    3) Do not mention any comparison with prior or earlier reports.
    4) Always give a single line report and dont introduce new line characters.
    5) Do not provide any special characters.
    ##

    ** 
    Stable appearance of lower cervical fusion Heart size normal. No pneumothorax or pleural effusion. No focal airspace disease. Calcified nodules consistent with chronic granulomatous disease. Bony structures appear intact.
    **
    """

    # Specify the model you want to use (e.g., "llama2", "mistral", etc.)
    model = "mistral"

    report_list = []

    for i in trange(len(df['findings'])):
        retrieved_documents = df['findings'].iloc[i].split('|')
        generated_report = generate_report(retrieved_documents, query, model)
        report_list.append(generated_report)
        # if i==100:
        #     break


    df1 = pd.DataFrame(report_list, columns=["findings"])
    df1.to_csv('final_prediction_report_mistral.csv',index=False)

