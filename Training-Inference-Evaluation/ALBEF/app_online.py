# Author 
# Rahul Kumar (Northeastern University)

import gradio as gr
import requests
import json
import pandas as pd
from tqdm import trange
import torch
import os
import cv2
from ALBEF import CXR_ReDonE_pipeline1



def get_ollama_completion(prompt, model="mistral"):
    """
    Send a prompt to a local Ollama instance and retrieve the generated completion.

    This function makes an HTTP POST request to a local server running an Ollama model. It sends a JSON
    payload containing the model type and the prompt, and returns the model's generated textual response.

    Args:
    prompt (str): The prompt text to send to the model.
    model (str): The model identifier, defaulting to 'mistral'.

    Returns:
    str: The text generated by the model in response to the prompt.
    """
    url = "http://localhost:11434/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    response = requests.post(url, json=data)
    return json.loads(response.text)['response']


def generate_report(retrieved_documents, model="mistral"):
    """
    Generate a detailed report based on provided documents and a query using an Ollama model.

    This function processes a list of documents, combines them into a single context, and constructs a
    prompt for an AI model to generate a report. The function utilizes the 'get_ollama_completion' function
    to get the AI-generated text based on the constructed prompt.

    Args:
    retrieved_documents (list[str]): A list of document texts retrieved as relevant information.
    query (str): The query specifying what the report should focus on.
    model (str): The Ollama model identifier to use for generating the report, defaulting to 'mistral'.

    Returns:
    str: A string containing the AI-generated report, formatted based on the input query and context.
    """
    # Combine retrieved documents into a single context
    context = "\n\n".join(retrieved_documents)

    # Define system and user prompts
    system_prompt = "You are an assistant designed to generate radiology reports based on provided context and query."

    user_prompt = f'''Based on the following context and query, generate a detailed report:

    Context:
    {context}

    Query: 
    {"""Please generate a concise radiology report with only impression section using information only from retrieved context. 
        ##
        Follow these guidelines to generate the report:
        1) Do not provide report by bullet points or numbers.
        2) Do not write the word impression, provide only the report. 
        3) Do not mention any comparison with prior or earlier reports.
        4) Always give a single line report and dont introduce new line characters.
        5) Do not provide any special characters.
        ##

        ** 
        Stable appearance of lower cervical fusion Heart size normal. No pneumothorax or pleural effusion. No focal airspace disease. Calcified nodules consistent with chronic granulomatous disease. Bony structures appear intact.
        **
    """}
    '''

    # Combine prompts and generate the report
    full_prompt = f"System: {system_prompt}\n\nHuman: {user_prompt}\n\nAssistant:"
    
    report = get_ollama_completion(full_prompt, model)
    
    return report.strip()

def inference(ChestImg):
    """
    Process an input chest X-ray image, generate findings using a pre-trained model, and produce a medical report.

    This function handles the image preprocessing, prediction, and report generation steps. It saves the
    input image to a designated directory, uses a pre-trained CXR model to predict findings, and then
    generates a detailed report based on these findings.

    Args:
    ChestImg (ndarray): A numpy array representing the chest X-ray image.

    Returns:
    str: A string containing the generated medical report based on the findings.
    """
    file_path = 'inference_images'
    os.makedirs(file_path, exist_ok=True)
    cv2.imwrite(os.path.join(file_path, "Input_image.png"), ChestImg)
    CXR_ReDonE_pipeline1.main()
    prediction = pd.read_csv('ALBEF/top_k_pred.csv')
    reports = prediction['findings']
    reports = reports.to_string(index=False)
    print(reports)
    split_reports = reports.split('|')
    return generate_report(split_reports)


with gr.Blocks() as demo:
    # Creating a layout for the Gradio interface with rows and columns
    with gr.Row():
        with gr.Column():
            # Defining an image input for chest X-rays with specified dimensions
            input_img = gr.Image(label="Chest X-Ray",height=400,width=600)
            # Button to trigger the report generation
            generate_btn = gr.Button("Generate Report")
        with gr.Column():
            # Textbox to display the generated report
            report = gr.Textbox(label="Report")

    # Link the button to the inference function, specifying input and output components
    generate_btn.click(fn=inference,inputs=input_img, outputs=report)

    # Providing example images for quick testing
    examples = gr.Examples(examples=['../../images/9_IM-2407-1001.dcm.png','../../images/1000_IM-0003-1001.dcm.png','../../images/1001_IM-0004-1002.dcm.png'],
            inputs=[input_img],label="Sample X-Ray scan")

# Launch the Gradio app with sharing options enabled
demo.launch(share=True)